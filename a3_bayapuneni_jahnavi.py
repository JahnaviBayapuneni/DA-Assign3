# -*- coding: utf-8 -*-
"""A3-Bayapuneni-Jahnavi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fkao2jlke2eVlNyyE6woBgmb2fzjk2Qn

#                                                      BREAST CANCER DIAGNOSTIC DATA SET



Student Name: Jahnavi Bayapuneni 
Mail: jahnavi.lucky67@gmail.com
Student ID : 200394366

##Part - A  Choosing the  Data Set 


Breast cancer is the most common cancer ecpecially for women. Approximately, one in eight will develop the breast cancer. We know that most of the people who actually get diagnosed in the country are gonna be in the early stage and if we take that tumor out, it may come as a surprise to many people but most of them in the world can be cured if they just went under the local therapy. 
So,there's oppertunities we could potentially learn, and help guide therapy, we could actually knew which patients were cured and which were not. 
Early detection of breast cancer can greatly improve pregnosis and survival chances by promoting clinical treatment to patients as soon as possible.
So, i think its really cool that you can possibly save someone's life from data.

This Data Set contains around 600 samples of information. Being a Data Science student, working with the small data set will help me to understand every possible outcome that i can get from this data. This results in better and expected outputs.

##Part B - Obtain the Data 



1.  Download the required csv files from the Data Set
2.  Upload the Data Set to the google drive
3.  import gdrive to the directory
"""

from google.colab import drive

drive.mount('/content/gdrive')

cd gdrive

cd My\ Drive

"""Importing the following libraries helps us to solve vaious problems in future"""

import numpy as np 
import pandas as pd
import os
from matplotlib import pyplot as plt

"""To use the required data for the program, I started with importing the csv file(assign.csv) from Kaggle and uploaded the file to my google drive. From there, i imported it through loading the google drive in the program and allowing the access to read the desired file using read_csv technique."""

data= pd.read_csv('assign.csv')

"""We have succesfully read the file assign.csv, printing the data displays us the data set confirming that we have succesfully imported the data set"""

data

"""## Part c - Scrubbing and formatting

We are dividing the Target and input variables from the data
"""

target_features=data["diagnosis"]
input_features = data.drop('diagnosis', axis=1)

"""Here, we are dropping irrelevant features from dataframe as column id. 

Column Id describes the uniqueness of each inpput. As we are not looking at each input individually for the data analysis in the perticular theme, and also there are other columns which play vital role to get the outputs we require, I feel like Column Id is not useful for analysis
"""

input_features = input_features.drop('id', axis=1)
i = len(input_features.columns)

"""Removing last column from the data."Unnamed:32" column is NaN . 

with this column being used or unused does not make any difference for the analysis of my data.
As,this column also does not play a crucial role in the output of data analysis we are looking for
i consider removing this column from the data  which is not useful for data analysis
"""

input_features = input_features.drop(input_features.columns[i-1], axis=1)

"""Here we are just replacing our target variable class into binary"""

target_features.replace(('M', 'B'), (1, 0), inplace=True)

input_features

"""## Part D - Exploratory Data Analysis

We are using info() to know the type of features we are dealing with in the data set.

By using input_features.info(), we got the result as non-null which means that there are no missing values in our data frame
"""

input_features.info()

"""ndim is useful to find the dimensions of our data set. Below, ndim returns 2 because we are using data frame which says 2-dimensional data."""

input_features.ndim

"""shape is used to find the shape of data i.e number of rows and columns we have in the data set."""

input_features.shape

"""head() is used to print the first rows of the dataset"""

input_features.head(5)

"""tail() is used to print the last rows of the data set"""

input_features.tail(5)

"""As we have already performed input_features.info(), from the result obtained from there and through by printing the few rows (top,down) from the data set, we can say that there is no presence of any anomolous data"""

input_features["radius_mean"].describe()  # Fourth part in part -D

"""For loop used below which helps us to print 5 number summery(min, lower quartile as 25%, median, upper quartile as 75%, max) for all the numeric data present in the dataset.

along with these, we also have count,mean and std
"""

cols=list(input_features.columns)
for name in cols:
  print(input_features[name].describe())

"""Here, we are displaying median value for the numeric data present in the data set 

input_features.median(axis=0) is used to print the value of median of all the columns present in the data.
"""

input_features.median(axis=0)

"""The  plot displayed below  is a scatter plot for texture_mean and perimeter_mean

We are using Scatter plot here because all the values are numeric. And scatter is a function which gives us scatter plot. 

From the plot displayed below, when x-axis is between 15 and 20 and y-axis between 60 and 90  there are more number of observations where x is texture_mean and y is perimeter_mean
"""

plt.scatter(input_features["texture_mean"],input_features["perimeter_mean"])

plt.show()

"""##PART  E - REPORT INITIAL FINDINGS 

With the data set exploring, the problem statement i choose is: 

From the data set we have, we are trying to predict whether the Breast Cancer is present in the person or not. From the data we have, we can detect it by using the radius, area how lengthy they are and how much place they are occupying. If the length or size of it tends to be more then it might result in detection of Breat cancer and if not then the output will be a no. 





> Modelling technique 



                                                                   
 Why Classification algorithm?

As our target variables are strings not numeric in the data set we are using, We need to execute the classification algorithm not the REgression algorithm. 

Here, For the dataset i have been taken, We are using the KNN classifier algorithm .

   Why KNN Algorithm ?

In the data set we are using , as our target variables are strings not numeric 

  Why the KNN  algorithms instead of others?

In classification model there are so many algorithms. But KNN is helpful in this case as our input variables are complete nummeric and its east to predict target variable by observing the K-nearest neighbours and calculating their euclidean distance and choosing the minimun one.

### References 



[1] UCI Machine Learning (2016). Breast Cancer Wisconsin (Diagnostic) Data Set. [online] Kaggle.com. Available at: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data#data.csv [Accessed 21 Aug. 2019].

â€Œ
"""